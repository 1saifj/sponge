[sponge](https://github.com/zhufuyi/sponge) is a microservice framework, a tool to quickly generate web and microservice code. sponge has a rich generating code commands, a total of 12 different functional code, these functional code can be combined into a complete service (similar to artificially broken sponge cells can be automatically reorganized into a new sponge). Microservice code features include logging, service registration and discovery, registry, rate limiter, circuit breaker, trace, metrics monitoring, pprof performance analysis, statistics, caching, CICD. Code decoupling modular design, including the complete project from development to deployment, common code and scripts are automatically generated, only in accordance with the code template to write business logic code, making the development efficiency improved a lot.

<br>

## 1 sponge function introduction

### 1.1 sponge command framework

The generated code is based on three approaches **Yaml**, **SQL DDL** and **Protocol buffers**, each possessing different functional code generation, and the framework diagram of the generated code is shown in Figure 1-1.

<p align="center">
<img width="1500px" src="https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/sponge-framework.png">
</p>

<p align="center">Figure 1-1 sponge generation code framework diagram</p>

<br>

- **Yaml** generates configuration files corresponding to go struct code.
- **SQL DDL** generated code includes http, handler, dao, model, proto, rpc, service, divided into two types of web and rpc, web and rpc types have their own sub-modules, each module code can be generated separately, between the modules like an onion layer of independent decoupling. The code includes standardized CRUD (add, delete, change, check) business logic, which can be directly compiled and used.
  - **web type code**: Generate http service code including handler, dao, model three sub-module code, as shown in the diagram inward contain, the same principle, generate handler module code contains dao, model two sub-module code.
  - **rpc type code**: generate rpc service code including service, dao, model, protocol buffers four sub-modules, as shown in the diagram inwards contain, generate service module code including dao, model, protocol buffers three sub-module code.
- The code generated by **Protocol buffers** includes http-pb, rpc-pb, rpc-gw-pb, also divided into web and rpc types, where http-pb, rpc-pb are usually used in combination with dao, model code generated by **SQL DDL**.
  - **http-pb**: http service code includes two sub-modules, router and handler template, excluding the operation database sub-module, and subsequent business logic code is filled to the handler template file.
  - **rpc-pb**: rpc service code includes service template a sub-module, not including the operation database module, subsequent business logic code filled to the service template file.
  - **rpc-gw-pb**: rpc gateway is actually a http service, including two submodules, router and service template, where the service template code is the business logic code related to calling the rpc service.

In the same folder, if the latest generated code is found to conflict with the original code, sponge will cancel the generation process and will not make any changes to the original code, so you don't have to worry about writing business logic code that is overwritten.

<br>

### 1.2 Microservices framework

The microservice code framework created by sponge is shown in Figure 1-2, This is a typical microservice hierarchy with high performance, high scalability, and includes common service governance features.

<p align="center">
<img width="1200px" src="https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/microservices-framework.png">
</p>

<p align="center">Figure 1-2 Microservices framework diagram</p>

<br>

Microservices main functions:

- Web framework [gin](https://github.com/gin-gonic/gin)
- RPC framework [grpc](https://github.com/grpc/grpc-go)
- Configuration file parsing [viper](https://github.com/spf13/viper)
- Configuration Center [nacos](https://github.com/alibaba/nacos)
- Logging [zap](https://go.uber.org/zap)
- Database component [gorm](https://gorm.io/gorm)
- Caching component [go-redis](https://github.com/go-redis/redis) [ristretto](github.com/dgraph-io/ristretto)
- Documentation [swagger](https://github.com/swaggo/swag)
- Authorization [jwt](https://github.com/golang-jwt/jwt)
- Validator [validator](https://github.com/go-playground/validator)
- Rate limiter [ratelimit](pkg/shield/ratelimit)
- Circuit Breaker [circuitbreaker](pkg/shield/circuitbreaker)
- Tracing [opentelemetry](https://go.opentelemetry.io/otel)
- Monitoring [prometheus](https://github.com/prometheus/client_golang/prometheus), [grafana](https://github.com/grafana/grafana)
- Statistics [gopsutil](https://github.com/shirou/gopsutil)
- Service registry and discovery [etcd](https://github.com/etcd-io/etcd), [consul](https://github.com/hashicorp/consul), [nacos](https://github.com/alibaba/)
- Performance analysis [go profile](https://go.dev/blog/pprof)
- Code inspection [golangci-lint](https://github.com/golangci/golangci-lint)
- Continuous Integration CI [jenkins](https://github.com/jenkinsci/jenkins)
- Continuous Deployment CD [docker](https://www.docker.com/), [kubernetes](https://github.com/kubernetes/kubernetes)

<br>

The code directory structure follows [project-layout](https://github.com/golang-standards/project-layout), and the code directory structure is shown below.

```bash
.
├── api               # Proto files and the generated *pb.go directory
├── assets            # Directory of other assets used with the repository (images, logos, etc.)
├── build             # Package and continuous integration directory
├── cmd               # Program entry directory
├── configs           # Directory for configuration files
├── deployments       # Configuration and template directories for IaaS, PaaS, system and container orchestrated deployments
├─ docs               # Design documentation and interface documentation directory
├── internal          # The code directory for private applications and libraries
│ ├── cache           # Business wrapper based cache directory
│ ├── config          # The configuration file directory for the Go structure
│ ├── dao             # Data access directory
│ ├── ecode           # Custom business error code directory
│ ├── handler         # Http's business function implementation directory
│ ├── model           # Database model directory
│ ├── routers         # Http routing directory
│ ├── rpcclient       # The client directory to connect to the rpc service
│ ├── server          # Service entry, including http, rpc, etc.
│ ├── service         # Rpc's business function implementation directory
│ └── types           # Directory of request and response types for http
├── pkg               # The directory of libraries that external applications can use
├── scripts           # A directory of scripts for performing various build, installation, analysis, etc. operations
├── test              # Additional external test procedures and test data
└── third_party       # External helpers, forked code, and other third-party tools
```

The web service and rpc service directory structures are basically the same, with some directories unique to the web service (routers, handlers, types in the internal directory) and some directories unique to the rpc service (services in the internal directory).

<br><br>

## 2 Install sponge and dependency tools

### 2.1 Window environment installation dependencies

If you use the windows environment, you need to install the relevant dependencies first, and just ignore the other environments.

**(1) Installing mingw64**

mingw64 stands for Minimalist GNUfor Windows, a freely available and freely distributed collection of Windows-specific header files and import libraries using the GNU toolset, download the pre-compiled source generated binaries at

https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win64/Personal%20Builds/mingw-builds/8.1.0/threads-posix/seh/x86_64-8.1.0-release-posix-seh-rt_v6-rev0.7z

After downloading and extracting to the `D:\Program Files\mingw64` directory, modify the system environment variable PATH to add `D:\Program Files\mingw64\bin`.

**Install the make command**

Switch to the `D:\Program Files\mingw64\bin` directory, find the `mingw32-make.exe` executable, copy it and rename it to `make.exe`.

<br>

**(2) Installation cmder**

**cmder** is an enhanced command line tool that contains some sponge dependent commands (bash, git, etc.), cmder download at

https://github.com/cmderdev/cmder/releases/download/v1.3.20/cmder.zip

After downloading and extracting to the `D:\Program Files\cmder` directory, modify the system environment variable PATH to add `D:\Program Files\cmder`.

<br>

### 2.2 Install sponge

**(1) Install go**

Download at https://go.dev/dl/ or https://golang.google.cn/dl/ Select version (>=1.16) to install, add `$GOROOT/bin` to the system path.

Note: If you don't have scientific internet access, it is recommended to set up a domestic proxy `go env -w GOPROXY=https://goproxy.cn,direct`

<br>

**(2) Install protoc**

Download it from https://github.com/protocolbuffers/protobuf/releases/tag/v3.20.3 and add the directory where the **protoc** file is located under systempath.

<br>

**(3) Install sponge**

> go install github.com/zhufuyi/sponge/cmd/sponge@latest

Note: The directory where the sponge binary is located must be under system path.

<br>

**(4) Install dependency plugins and tools**

> sponge init

Dependency plugins and tools are automatically installed after executing the command: [protoc-gen-go](https://google.golang.org/protobuf/cmd/protoc-gen-go), [protoc-gen-go-grpc](https://google.golang.org/grpc/cmd/protoc-gen-go-grpc), [protoc-gen-validate](https://github.com/envoyproxy/protoc-gen-validate), [protoc-gen-gotag](https://github.com/srikrsna/protoc-gen-gotag), [protoc-gen-go-gin](https://github.com/zhufuyi/sponge/cmd/protoc-gen-go-gin), [protoc-gen-go-rpc-tmpl](https://github.com/zhufuyi/sponge/cmd/protoc-gen-go-rpc-tmpl), [protoc-gen-openapiv2](https://github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2), [protoc-gen-doc](https://github.com/pseudomuto/protoc-gen-doc/cmd/protoc-gen-doc), [golangci-lint](https://github.com/golangci/golangci-lint/cmd/golangci-lint), [swag](https://github.com/swaggo/swag/cmd/swag), [go-callvis](https://github.com/ofabry/go-callvis).

If there is a dependency tool installation error, execute the command to retry.

> sponge tools --install

To view the installation of dependency tools.

```bash
sponge tools
```

List of all dependent tools.

```
Installed dependency tools:
    ✔  go
    ✔  protoc
    ✔  protoc-gen-go
    ✔  protoc-gen-go-grpc
    ✔  protoc-gen-validate
    ✔  protoc-gen-gotag
    ✔  protoc-gen-go-gin
    ✔  protoc-gen-go-rpc-tmpl
    ✔  protoc-gen-openapiv2
    ✔  protoc-gen-doc
    ✔  swag
    ✔  golangci-lint
    ✔  go-callvis
```

<br>

The help information for the **sponge** command has detailed usage examples, add `-h` to the end of the command to see, for example `sponge web model -h`, which is the help information returned by generating the model code for gorm based on the mysql table.

<br>

### 2.3 Command UI for sponge

After installing sponge and the dependency tools, you can start using it. sponge provides two ways to generate code, command line and UI interface, in fact, through the UI interface way, in the background is also to execute commands. sponge UI interface supports the memory function of executing commands, it is more convenient to use, start UI service: ``bash

```bash
sponge run
```

Visit `http://localhost:24631` in the browser, open the homepage as shown in Figure 2-1, generate the code according to the actual need, the code is api interface downloaded to the local, so you can also deploy sponge's UI service to the server for long-term operation.

![home](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/home.png)
*Figure2-1 sponge ui diagram*

<br><br>

## 3 Quickly create web projects

### 3.1 Create http service from mysql

#### 3.1.1 Creating a table

To generate code based on mysql's data table, first prepare a mysql service ([docker install mysql](https://github.com/zhufuyi/sponge/blob/main/test/server/mysql/docker-compose.yaml)). For example, mysql has a database school and a data table teacher under the database, as shown in the following sql.

```sql
CREATE DATABASE IF NOT EXISTS school DEFAULT CHARSET utf8mb4 COLLATE utf8mb4_unicode_ci;

use school;

create table teacher
(
    id bigint unsigned auto_increment
        primary key,
    created_at datetime null,
    updated_at datetime null,
    deleted_at datetime null,
    name varchar(50) not null comment 'username',
    password varchar(100) not null comment 'password',
    email varchar(50) not null comment 'mail',
    phone varchar(30) not null comment 'mobile phone number',
    avatar varchar(200) null comment 'avatar',
    gender tinyint not null comment 'gender, 1:male, 2:female, other values:unknown',
    age tinyint not null comment 'age',
    birthday varchar(30) not null comment 'date of birth',
    school_name varchar(50) not null comment 'school name',
    college varchar(50) not null comment 'college',
    title varchar(10) not null comment 'title',
    profile text not null comment 'personal profile'
)
    comment 'teacher';

create index teacher_deleted_at_index
    on teacher (deleted_at);
```

Import the SQL DDL into mysql to create a database school, with a table teacher under school.

<br>

#### 3.1.2 Generating http service code

Open a terminal and execute the command.

```bash
sponge web http \
  --module-name=edusys \
  --server-name=edusys \
  --project-name=edusys \
  --repo-addr=zhufuyi \
  --db-dsn=root:123456@(192.168.3.37:3306)/school \
  --db-table=teacher \
  --out=./edusys
```

Check the parameter description command `sponge web http -h`, note that the parameter **repo-addr** is the image repository address, if you use the [official docker image repository](https://hub.docker.com/), you only need to fill in the username of the registered docker repository, if you use the private repository address, you need to fill in the full repository address.

<br>

Generating the complete http service code is in the current directory edusys with the following directory structure.

```
.
├── build
├── cmd
│    └── edusys
│          └── initial
├── configs
├── deployments
│    ├── docker-compose
│    └── kubernetes
├── docs
├── internal
│    ├── cache
│    ├── config
│    ├── dao
│    ├── ecode
│    ├── handler
│    ├── model
│    ├── routers
│    ├── server
│    └── types
└── scripts
```

The Makefile file in the edusys directory, which integrates commands related to compiling, testing, running, and deploying, switches to the edusys directory to run the service at:

```bash
# Update swagger documentation
make docs

# Compile and run services
make run
```

Copy http://localhost:8080/swagger/index.html to your browser to test the CRUD api, as shown in Figure 3-1.

![http-swag](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/http-swag.jpg)

*Figure 3-1 http swagger documentation diagram*

<br>

By default, the service is only enabled for the metrics collection api, per-minute resource statistics information, and other service governance is off by default. In practical applications, some adjustments are made as needed.

- To use redis as a cache, open the configuration file `configs/edusys.yml`, change the **cacheType** field value to redis, and fill in the **redis** configuration address and port.
- By default, rate limiter, circuit breaker, trace, service registration and discovery functions are off, you can open the configuration file `configs/edusys.yml` to turn on the relevant functions, if you turn on the trace function, you must fill in the jaeger configuration information; if you turn on the service registration and discovery function, you must fill in one of the consul, etcd, nacos configuration information.
- If a configuration field name is added or modified, execute the command `sponge config --server-dir=./edusys` to update the corresponding go struct; it is not necessary to execute the update command to modify only the field values.
- Modify the error code information corresponding to the CRUD api, open `ingernal/ecode/teacher_http.go`, modify the variable **teacherNO** value, which is the only value that does not repeat, the return message description is modified according to your needs, the api custom error codes for the teacher table operations are added here.

<br>

#### 3.1.3 Generating Handler Code

In a service, there is usually more than one data table, so if a new data table is added, how can the generated handler code be automatically populated into the existing service code, using the `sponge web handler` command, for example if two new data tables **course** and **teach** are added.

```sql
create table course
(
    id bigint unsigned auto_increment
        primary key,
    created_at datetime null,
    updated_at datetime null,
    deleted_at datetime null,
    code varchar(10) not null comment 'course code',
    name varchar(50) not null comment 'course name',
    credit tinyint not null comment 'credits',
    college varchar(50) not null comment 'college',
    semester varchar(20) not null comment 'semester',
    time varchar(30) not null comment 'class time',
    place varchar(30) not null comment 'place of class'
)
    comment 'course';

create index course_deleted_at_index
    on course (deleted_at);


create table teach
(
    id bigint unsigned auto_increment
        primary key,
    created_at datetime null,
    updated_at datetime null,
    deleted_at datetime null,
    teacher_id bigint not null comment 'teacher id',
    teacher_name varchar(50) not null comment 'teacher name',
    course_id bigint not null comment 'course id',
    course_name varchar(50) not null comment 'course name',
    score char(5) not null comment 'students evaluate the quality of teaching, 5 grades: A,B,C,D,E'
)
    comment 'teacher course';

create index teach_course_id_index
    on teach (course_id);

create index teach_deleted_at_index
    on teach (deleted_at);

create index teach_teacher_id_index
    on teach (teacher_id);
```

<br>

Generate handler code that contains CRUD business logic.

```bash
sponge web handler \
  --db-dsn=root:123456@(192.168.3.37:3306)/school \
  --db-table=course,teach \
  --out=./edusys
```

Check the parameter description command `sponge web handler -h`, the parameter `out` is to specify the existing service folder edusys, if the parameter `out` is empty, you must specify the `module-name` parameter, generate the handler submodule code in the current directory, then copy the handler code to the folder edusys, the effect of both ways are The effect is the same.

After executing the command, the course and teach related code is generated in the `edusys/internal` directory.

```
.
└── internal
      ├── cache
      ├── dao
      ├── ecode
      ├── handler
      ├── model
      ├── routers
      └── types
```

<br>

Switch to the edusys directory and execute the command to run the service.

```bash
# Update swagger documentation
make docs

# Compile and run services
make run
```

Copy http://localhost:8080/swagger/index.html to your browser to test the CRUD api, as shown in Figure 3-2.

![http-swag2](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/http-swag2.jpg)

*Figure 3-2 http swagger documentation diagram*

The actual use requires modifying the custom CRUD api to return error codes and messages, opening the file `ingernal/ecode/course_http.go` to modify the variable **courseNO** value, and opening the file `ingernal/ecode/teach_http.go` to modify the variable **teachNO** values.

Although the CRUD api of each data table is generated, it is not necessarily suitable for the actual business logic, so you need to add the business logic code manually, fill in the database operation code to the `internal/dao` directory, and the business logic code to the `internal/handler` directory.

<br>

### 3.2 Create http service from proto file

If the standard CRUD api http service code is not required, you can customize the api in the proto file and use the spong command to generate the http service and api template code.

#### 3.2.1 Custom apis

The following is a sample file, teacher.proto, where each method defines the description of the route and swagger document. The tag and validate descriptions are added to the message as needed.

```protobuf
syntax = "proto3";

package api.edusys.v1;

import "google/api/annotations.proto";
import "protoc-gen-openapiv2/options/annotations.proto";

option go_package = "edusys/api/edusys/v1;v1";

// Generate *.swagger.json basic information
option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_swagger) = {
  host: "localhost:8080"
  base_path: ""
  info: {
    title: "edusys api docs";
    version: "v0.0.0";
  };
  schemes: HTTP;
  schemes: HTTPS;
  consumes: "application/json";
  produces: "application/json";
};

service teacher {
  rpc Register(RegisterRequest) returns (RegisterReply) {
    // Set up routing
    option (google.api.http) = {
      post: "/api/v1/Register"
      body: "*"
    };
    // Set the swagger document corresponding to the route
    option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_operation) = {
      summary: "Registered Users",
      description: "Submit information for registration",
      tags: "teacher",
    };
  }

  rpc Login(LoginRequest) returns (LoginReply) {
    option (google.api.http) = {
      post: "/api/v1/login"
      body: "*"
    };
    option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_operation) = {
      summary: "Login",
      description: "Login",
      tags: "teacher",
    };
  }
}

message RegisterRequest {
  string name = 1;
  string email = 2;
  string password = 3;
}

message RegisterReply {
  int64 id = 1;
}

message LoginRequest {
  string email = 1;
  string password = 2;
}

message LoginReply {
  string token = 1;
}
```

<br>

#### 3.2.2 Generating http service code

Open a terminal and execute the command.

```bash
sponge web http-pb \
  --module-name=edusys \
  --server-name=edusys \
  --project-name=edusys \
  --repo-addr=zhufuyi \
  --protobuf-file=./teacher.proto \
  --out=./edusys
```

Check the parameter description command `sponge web http-pb -h`, which supports \* sign matching (example `--protobuf-file=*.proto`), indicating that code is generated based on a bulk proto file, and multiple proto files include at least one service, otherwise code generation is not allowed.

The directory for generating http service code is shown below, there are some differences with the http service code directory generated by `sponge web http`, the new proto file related **api** and **third_party** directories are added, there is no **cache**, **dao**, **model**, **types** directories in the internal directory. **handler**, **types** directories, where **handler** is the directory that holds business logic template code, which will be automatically generated by command.

```
.
├── api
│    └── edusys
│          └──v1
├── build
├── cmd
│    └── edusys
│          └── initial
├── configs
├── deployments
│    ├── docker-compose
│    └── kubernetes
├── docs
├── internal
│    ├── config
│    ├── ecode
│    ├── routers
│    └── server
├── scripts
└── third_party
```

Switch to the edusys directory and execute the command to run the service.

```bash
# Generate *pb.go file, generate handler template code, update swagger documentation
make proto

# Compile and run services
make run
```

Copying http://localhost:8080/apis/swagger/index.html to the browser test api, as shown in Figure 3-3, the request returns a 500 error because the template code (internal/handler/teacher_logic.go file) calls `panic("implement me")` directly, which is meant to prompt for business logic code to be filled in.

![http-pb-swag](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/http-pb-swag.jpg)
*Figure 3-3 http swagger documentation diagram*

<br>

#### 3.2.3 Adding a new api

Depending on the business requirements, new apis need to be added, in two cases.

**(1) Add new api to original proto file**

Open `api/edusys/v1/teacher.proto`, e.g. add the **bindPhone** method and fill in the routing and swagger documentation description information to finish adding a new api.

Execution order.

```bash
# Generate *pb.go file, generate handler template code, update swagger documentation
make proto
```

Generate new template files in the `internal/handler` and `internal/ecode` directories, then copy the latest generated template code into the business logic code area at.

- The template code file with the suffix **.gen.datetime** is generated in the `internal/handler` directory (example teacher_logic.go.gen.xxxx225619), because teacher_logic.go already exists and will not overwrite the business logic code written, so a new file is generated. Open the file teacher_logic.go.gen.xxxx225619, copy the template code for the add method **bindPhone** api to the teacher_logic.go file, and fill in the business logic code.
- The template code file with the suffix **.gen.datetime** is generated in the `internal/ecode` directory, and the **bindPhone** api error code is copied into the teacher_http.go file.
- Delete all files with the suffix **.gen.datetime**.

<br>

**(2) Adding apis to new proto files**

For example, if a new **course.proto** file is added, the api under **course.proto** must include the routing and swagger documentation description information, check **Chapter 3.2.1** and copy the **course.proto** file to the `api/edusys/v1` directory to complete the newly added api.

Execution order.

```bash
# Generate *pb.go file, generate handler template code, update swagger documentation
make proto
```

Generate code files with the **course** name prefix in the `internal/handler`, `internal/ecode`, and `internal/routers` directories by doing the following two operations.

- Fill in the business code in the `internal/handler/course.go` file.
- Modify the custom error code and message description in the `internal/ecode/course_http.go` file.

<br>

#### 3.2.4 Refining the http service

The http service code generated by the `sponge web http-pb` command does not have code related to `dao`, `cache`, `model` and other manipulation data, users can implement it themselves, if you use mysql database and redis cache, you can use **sponge** tool to generate `dao`, `cache`, `model` code directly.

Generate CRUD operation database code command.

```bash
sponge web dao \
  --db-dsn=root:123456@(192.168.3.37:3306)/school \
  --db-table=teacher \
  --include-init-db=true \
  --out=./edusys
```

Check the parameter description command `sponge web dao -h`, the parameter `-include-init-db` is used only once in a service, remove the parameter `-include-init-db` the next time you generate `dao` code, otherwise it will result in not generating the latest `dao` code, because the db initialization code already exists.

Whether you implement the `dao` code yourself or use the `dao` code generated by sponge, there are a number of operations that need to be done afterwards.

- Add mysql and redis to the initialization and release resource code of the service, open the `cmd/edusys/initial/initApp.go` file, backcomment out the call to mysql and redis initialization code, open the `cmd/edusys/initial/registerClose.go` file , backcomment out the call to mysql and redis release resource code, the initial code is a one-time change.
- The generated `dao` code, and custom methods **register** and **login** can not correspond exactly, you need to manually in the file `internal/dao/teacher.go` to supplement the code (file name teacher is the name of the table), and then in the `internal/handler/teacher. go` to fill in the business logic code (filename teacher is the name of the proto file), the business code returns the error using the error code defined in the `internal/ecode` directory, if the error message is returned directly, the requesting side will receive an UNKNOWN error message, that is, an undefined error message.
- The default uses local memory for caching, change it to use redis as cache, change the field **cacheType** value to redis in the configuration file `configs/edusys.yml`, and fill in the redis address and port.

Switching to the edusys directory to run the service again.

```bash
go mod tidy

# Compile and run services
make run
```

Open `http://localhost:8080/apis/swagger/index.html` to request the api again, and it returns data properly.

<br>

### 3.3 Summary

There are two ways to generate http service code, mysql and proto files.

- According to mysql generated http service code includes CRUD api code for each data table, subsequent addition of new apis, you can refer to the CRUD way to add business logic code, the newly added apis need to manually fill in the swagger description information.
- The http service generated from the proto file does not include the manipulation database code though, nor the CRUD api logic code, which can be generated using the `sponge web dao` command to manipulate the database code as needed. With the addition of the new api, in addition to generating handler template code, swagger documentation, route registration code, and error codes for the api are automatically generated.

Both ways can complete the same http service api, according to the actual application choose one of them, if you do backend management services, use mysql to produce CRUD api code directly, you can write less code. For most need to customize the api service, use the proto file way to generate the http service, this way is also more freedom, after writing the proto file, in addition to the business logic code, other code is generated through the plug-in.

<br><br>

## 4 Quick creation of microservices

### 4.1 Create rpc service from mysql

#### 4.1.1 Generating rpc service code

Using the TEACHER table in **Section 3.1.1** as an example, create the rpc service.

```bash
sponge micro rpc \
  --module-name=edusys \
  --server-name=edusys \
  --project-name=edusys \
  --repo-addr=zhufuyi \
  --db-dsn=root:123456@(192.168.3.37:3306)/school \
  --db-table=teacher \
  --out=./edusys
```

Check the parameter description command `sponge micro rpc -h`, which generates the rpc service code in the current edusys directory with the following directory structure.

```
.
├── api
│    ├── edusys
│    │    └── v1
│    └── types
├── build
├── cmd
│    └── edusys
│          └── initial
├── configs
├── deployments
│    ├── docker-compose
│    └── kubernetes
├── docs
├── internal
│    ├── cache
│    ├── config
│    ├── dao
│    ├── ecode
│    ├── model
│    ├── server
│    └── service
├── scripts
└── third_party
```

The Makefile file in the edusys directory integrates commands related to compiling, testing, running, deploying, etc. Switch to the edusys directory and execute the command to run the service:.

```bash
# Generate *pb.go
make proto

# Compile and run services
make run
```

The rpc service includes the CRUD logic code as well as the rpc client test and pressure test code, using **Goland** or **VS Code** to open the `internal/service/teacher_client_test.go` file

- For tests of methods under **Test_teacherService_methods**, fill in the test parameters before testing.
- Execute the method pressure test under **Test_teacherService_benchmark**, fill in the pressure test parameters before testing, generate the pressure test report after execution, and copy the pressure test report file path to the browser to view the statistics, as shown in Figure 4-1.

![performance-test](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/performance-test.jpg)
*Figure 4-1 Performance test reporting diagram*

From the service startup log, you see that the default listening on port **8282** (rpc service) and port **8283** (collecting metrics or profiles) is turned on to print resource statistics per minute. In practice, some modifications are made as needed.

- To use redis as a cache, open the configuration file `configs/edusys.yml`, change the **cacheType** field value to redis, and fill in the **redis** configuration address and port.
- By default, the rate limiter, circuit breaker, trace, service registration and discovery functions are off, you can open the configuration file `configs/edusys.yml` to turn on the relevant functions, if you turn on the trace function, you need to fill in the jaeger configuration information, if you turn on the service registration and discovery function, you need to fill in one of the consul, etcd, nacos configuration information.
- If a configuration field name is added or modified, execute the command `sponge config --server-dir=./edusys` to update the corresponding go struct; you do not need to execute the update command to modify only the field values.
- Modify the error code and error message corresponding to the CRUD method, open `ingernal/ecode/teacher_rpc.go`, modify the variable **teacherNO** value (the value is unique), the return message description is modified according to your needs, the api error messages for the teacher table operations are added here.

<br>

#### 4.1.2 Generating service code

Two new tables course and teach were added, the structure of the data table is shown in section **3.1.3** and the service code was generated.

```bash
sponge micro service \
  --db-dsn=root:123456@(192.168.3.37:3306)/school \
  --db-table=course,teach \
  --out=./edusys
```

View the parameter description command `sponge micro service -h`, parameter `out` specifies the existing rpc service folder edusys, if parameter `out` is empty, you must specify the `module-name` and `server-name` parameters, generate the service code in the current directory, and then manually copy it to the folder edusys, the effect of both ways is The effect is the same.

After executing the command, the course and teach related code is generated in the following directory, and if you add custom methods or new protocol buffers files, the code is also added manually in the following directory.

```
.
├── api
│    └── edusys
│          └── v1
└── internal
      ├── cache
      ├── dao
      ├── ecode
      ├── model
      └── service
```

<br>

Switch to the edusys directory and execute the command to run the service.

```bash
# Update *.pb.go
make proto

# Compile and run services
make run
```

Open the `internal/service/course_client_test.go` and `internal/service/teach_client_test.go` files using **Goland** or **VS Code** to test the CRUD methods, you need to fill in the parameters before testing.

<br>

### 4.2 Creating rpc services from proto files

sponge not only supports creating rpc services based on mysql, but also supports generating rpc services based on proto files.

#### 4.2.1 Custom Methods

The following is a sample proto file teacher.proto Contents.

```protobuf
syntax = "proto3";

package api.edusys.v1;
option go_package = "edusys/api/edusys/v1;v1";

service teacher {
  rpc Register(RegisterRequest) returns (RegisterReply) {}
  rpc Login(LoginRequest) returns (LoginReply) {}
}

message RegisterRequest {
  string name = 1;
  string email = 2;
  string password = 3;
}

message RegisterReply {
  int64 id = 1;
}

message LoginRequest {
  string email = 1;
  string password = 2;
}

message LoginReply {
  string token = 1;
}
```

<br>

#### 4.2.2 Generating rpc service code

Open a terminal and execute the command.

```bash
sponge micro rpc-pb \
  --module-name=edusys \
  --server-name=edusys \
  --project-name=edusys \
  --repo-addr=zhufuyi \
  --protobuf-file=./teacher.proto \
  --out=./edusys
```

Check the parameter description command `sponge micro rpc-pb -h`, which supports \* sign matching (example `--protobuf-file=*.proto`), indicating that code is generated based on a bulk proto file, and that multiple proto files include at least one service, otherwise no code can be generated.

The generated rpc service code directory is shown below, there are some differences with the rpc service code directory generated by `sponge micro rpc`, there are no **cache**, **dao**, **model** subdirectories under the internal directory.

```
.
├── api
│    └── edusys
│          └── v1
├── build
├── cmd
│    └── edusys
│          └── initial
├── configs
├── deployments
│    ├── docker-compose
│    └── kubernetes
├── docs
├── internal
│    ├── config
│    ├── ecode
│    ├── server
│    └── service
├── scripts
└── third_party
```

Switch to the edusys directory and execute the command to run the service.

```bash
# Generate *pb.go file, generate service template code
make proto

# Compile and run services
make run
```

After starting the rpc service, use **Goland** or **VS Code** to open the `internal/service/teacher_client_test.go` file and test each method under **Test_teacher_methods**, fill in the test parameters before testing, you will find that the request returns an internal error, because in the template code file ` internal/service/teacher.go` (the filename teacher is the proto filename) inserts the code `panic("implement me")`, which is meant to prompt for filling in the business logic code.

<br>

#### 4.2.3 Adding new methods

Depending on the business requirements, new methods need to be added, operating in two cases.

**(1) Add new method to original proto file**

Open `api/edusys/v1/teacher.proto` and add the **bindPhone** method, for example.

Execution order.

```bash
# Generate *pb.go file, generate service template code
make proto
```

Generate the template code in the `internal/service` and `internal/ecode` directories, then copy the template code to the business logic code area at.

- The template code file with the suffix **.gen.datetime** is generated in the `internal/service` directory (example teacher.go.gen.xxxx225732), because teacher.go already exists and will not overwrite the business logic code originally written, so a new file is generated, open the file teacher.go.gen.xxxx225732, copy the template code that adds the **bindPhone** method to the teacher.go file, and then fill in the business logic code.
- The file with the suffix **teacher_rpc.go.gen.datetime** is generated in the `internal/ecode` directory, and the error code corresponding to the **bindPhone** method is copied into the teacher_rpc.go file.
- Delete all files with the suffix **.gen.datetime**.

<br>

**(2) Add new method to new proto file**

For example, if a new **course.proto** file is added, copy the **course.proto** file to the `api/edusys/v1` directory to complete the newly added api.

Execution order.

```bash
# Generate *pb.go file, generate service template code
make proto
```

Generate code files with the **course** name prefix in the `internal/service`, `internal/ecode`, and `internal/routers` directories by doing the following two operations.

- Fill in the business code in the `internal/service/course.go` file.
- Modify the custom error code and message description in the `internal/ecode/course_rpc.go` file.

<br>

#### 4.2.4 Refining the rpc service code

The rpc service code generated by the `sponge micro rpc-pb` command does not have code related to `dao`, `cache`, `model` and other manipulation data, users can implement it themselves, if you use mysql database and redis cache, you can use **sponge** tool to generate `dao`, `cache`, `model` code directly.

Generate CRUD operation database code command.

```bash
sponge micro dao \
  --db-dsn=root:123456@(192.168.3.37:3306)/school \
  --db-table=teacher \
  --include-init-db=true \
  --out=./edusys
```

Check the parameter description command `sponge micro dao -h`, the parameter `-include-init-db` is used only once in a service, remove the parameter `-include-init-db` the next time you generate `dao` code, otherwise it will cause the latest `dao` code to not be generated.

Whether you implement the `dao` code yourself or use the `dao` code generated by sponge, there are a number of operations that need to be done afterwards.

- Add mysql and redis to the initialization and release resource code of the service, open the `cmd/edusys/initial/initApp.go` file, backcomment out the call to mysql and redis initialization code, open the `cmd/edusys/initial/registerClose.go` file , backcomment out the call to mysql and redis release resource code, the initial code is a one-time change.
- The generated `dao` code, and custom methods **register** and **login** can not correspond exactly, you need to manually in the file `internal/dao/teacher.go` to supplement the code (file name teacher is the name of the table), and then in the `internal/handler/teacher. go` to fill in the business logic code (filename teacher is the name of the proto file), the business code returns the error using the error code defined in the `internal/ecode` directory, if the error message is returned directly, the requesting side will receive an UNKNOWN error message, that is, an undefined error message.
- The default uses local memory for caching, change it to use redis as cache, change the field **cacheType** value to redis in the configuration file `configs/edusys.yml`, and fill in the redis address and port.

Switching to the edusys directory to run the service again.

```bash
go mod tidy

# Compile and run services
make run
```

After starting the rpc service, use **Goland** or **VS Code** to open the `internal/service/teacher_client_test.go` file to test each method.

<br>

### 4.3 Create rpc gateway service from proto file

Microservices usually provide fine-grained APIs, and the actual APIs provided to the client are coarse-grained APIs that require data from different microservices to be aggregated together to form an API that meets the actual requirements, which is the role of the rpc gateway. rpc gateway itself is also an http service, as shown in Figure 4-2.

![rpc-gateway](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/rpc-gateway.png)
*Figure 4-2 rpc gateway framework diagram*

<br>

#### 4.3.1 Defining protocol buffers

Take e-commerce microservices as an example, the product detail page has information such as product, inventory, and product evaluation, which are stored in different microservices, and generally rarely request each microservice to get the data, direct requests to microservices will cause network pressure to multiply, and the usual practice is to aggregate multiple microservice data to return at once.

The following four folders have a simple proto file under each folder.

- **comment**: proto directory of comment services
- **inventory**: proto directory for inventory services
- **product**: proto directory of products and services
- **shopgw**: proto directory for rpc gateway services

```
.
├── comment
│    └── v1
│          └──comment.proto
├── inventory
│    └── v1
│          └── inventory.proto
├── product
│    └── v1
│          └── product.proto
└── shopgw
      └── v1
            └── shopgw.proto
```

The **comment.proto** file reads as follows.

```protobuf
syntax = "proto3";

package api.comment.v1;

option go_package = "shopgw/api/comment/v1;v1";

service Comment {
  rpc ListByProductID(ListByProductIDRequest) returns (ListByProductIDReply) {}
}

message ListByProductIDRequest {
  int64 productID = 1;
}

message CommentDetail {
  int64 id=1;
  string username = 2;
  string content = 3;
}

message ListByProductIDReply {
  int32 total = 1;
  int64 productID = 2;
  repeated CommentDetail commentDetails = 3;
}
```

<br>

The **inventory.proto** file reads as follows.

```protobuf
syntax = "proto3";

package api.inventory.v1;

option go_package = "shopgw/api/inventory/v1;v1";

service Inventory {
  rpc GetByID(GetByIDRequest) returns (GetByIDReply) {}
}

message GetByIDRequest {
  int64 id = 1;
}

message InventoryDetail {
  int64 id = 1;
  float num = 4;
  int32 soldNum =3;
}

message GetByIDReply {
  InventoryDetail inventoryDetail = 1;
}
```

<br>

The **product.proto** file reads as follows.

```protobuf
syntax = "proto3";

package api.product.v1;

option go_package = "shopgw/api/product/v1;v1";

service Product {
  rpc GetByID(GetByIDRequest) returns (GetByIDReply) {}
}

message GetByIDRequest {
  int64 id = 1;
}

message ProductDetail {
  int64 id = 1;
  string name = 2;
  float price = 3;
  string description = 4;
}

message GetByIDReply {
  ProductDetail productDetail = 1;
  int64 inventoryID = 2;
}
```

<br>

The contents of the **shopgw.proto** file are as follows. The proto for the rpc gateway service is a little different from the proto for other microservices in that you need to specify the method's routing and swagger description information.

```protobuf
syntax = "proto3";

package api.shopgw.v1;

import "api/product/v1/product.proto";
import "api/comment/v1/comment.proto";
import "api/inventory/v1/inventory.proto";
import "google/api/annotations.proto";
import "protoc-gen-openapiv2/options/annotations.proto";

option go_package = "shopgw/api/shopgw/v1;v1";

// default settings for generating *.swagger.json documents
option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_swagger) = {
  host: "localhost:8080"
  base_path: ""
  info: {
    title: "eshop api docs";
    version: "v0.0.0";
  };
  schemes: HTTP;
  schemes: HTTPS;
  consumes: "application/json";
  produces: "application/json";
};

service ShopGw {
  rpc GetDetailsByProductID(GetDetailsByProductIDRequest) returns (GetDetailsByProductIDReply) {
    option (google.api.http) = {
      get: "/api/v1/detail"
    };
    option (grpc.gateway.protoc_gen_openapiv2.options.openapiv2_operation) = {
      summary: "get detail",
      description: "get detail from product id",
      tags: "shopgw",
    };
  }
}

message GetDetailsByProductIDRequest {
  int64 productID = 1;
}

message GetDetailsByProductIDReply {
  api.product.v1.ProductDetail productDetail = 1;
  api.inventory.v1.InventoryDetail inventoryDetail = 2;
  repeated api.comment.v1.CommentDetail commentDetails = 3;
}
```

<br>

#### 4.3.2 Generating rpc gateway service code

Generate the rpc gateway service code from the **shopgw.proto** file.

```bash
sponge micro rpc-gw-pb \
  --module-name=shopgw \
  --server-name=shopgw \
  --project-name=eshop \
  --repo-addr=zhufuyi \
  --protobuf-file=./shopgw/v1/shopgw.proto \
  --out=./shopgw
```

Viewing the parameter description command `sponge micro rpc-gw-pb -h`, the generated rpc gateway service code is in the current shopgw directory with the following directory structure.

```
.
├── api
│    └── shopgw
│          └── v1
├── build
├── cmd
│    └── shopgw
│          └── initial
├── configs
├── deployments
│    ├── docker-compose
│    └── kubernetes
├── docs
├── internal
│    ├── config
│    ├── ecode
│    ├── routers
│    ├── rpcclient
│    └── server
├── scripts
└── third_party
```

Since **product.proto** depends on the files **product.proto**, **inventory.proto**, **comment.proto**, copy the three dependent proto files to the api directory, the api directory structure is as follows.

```
.
├── comment
│    └── v1
│          └── comment.proto
├── inventory
│    └── v1
│          └── inventory.proto
├── product
│    └── v1
│          └── product.proto
└── shopgw
      └── v1
            └── shopgw.proto
```

<br>

Switching to the shopgw directory to run the service.

```bash
# Generate *pb.go files, generate template code, update swagger documentation
make proto

# Compile and run services
make run
```

Copy http://localhost:8080/apis/swagger/index.html to the browser to test the api, as shown in Figure 4-3. The request returns a 500 error because the template code (internal/service/shopgw_logic.go file) calls `panic("implement me")` directly, which is meant to prompt for business logic code to be filled in.

![rpc-gw-swag](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/rpc-gw-swag.jpg)
*Figure 4-3 swagger documentation api for rpc gateway diagram*

<br>

#### 4.3.3 Refining the rpc gateway service code

**(1) Generate code to connect to the rpc server**

The service does not yet have a connection to the rpc service code, and the following is the command that generates the client code to connect to the **product**, **inventory**, and **comment** rpc services.

```bash
sponge micro rpc-cli \
  --rpc-server-name=comment,inventory,product \
  --out=./shopgw
```

View the parameter description command `sponge micro rpc-cli -h`, the parameter `out` specifies the existing service folder shopgw, and the generated code is in the `internal/rpcclent` directory.

<br>

**(2) Initializing and closing rpc connections**

The connection rpc server code includes initialization and shutdown functions, which are filled in according to the calling template code at

- Initialize when starting the service, under the code segment `// initializing the rpc server connection` in the `cmd/shopgw/initial/initApp.go` file, calling the initialization function based on the template.
- To release resources when closing the service, the release function is called according to the template in the `cmd/shopgw/initial/registerClose.go` file under the code segment `// close the rpc client connection`.

<br>

**(3) Modification of configuration**

Connection **product**, **inventory**, **comment** three rpc service code has been available, but the rpc service address has not been configured, you need to add in the configuration file `configs/shopgw.yml` under the field `grpcClient` to connect product, inventory , comment three microservice configuration information.

```yaml
grpcClient:
  - name: "product"
    host: "127.0.0.1"
    port: 8201
    registryDiscoveryType: ""
  - name: "inventory"
    host: "127.0.0.1"
    port: 8202
    registryDiscoveryType: ""  
  - name: "comment"
    host: "127.0.0.1"
    port: 8203
    registryDiscoveryType: ""
```

If the rpc service uses registration and discovery, the field `registryDiscoveryType` fills in the service registration and discovery type, which supports consul, etcd, and nacos.

Generate the corresponding go struct code.

```bash
sponge config --server-dir=./shopgw
```

<br>

**(4) Fill in the operational code**

The following is a sample business logic code filled in the template file `internal/service/shopgw_logic.go` to fetch data from **product**, **inventory**, **comment** three rpc services respectively aggregated together and returned.

```go
package service

import (
	"context"

	commentV1 "shopgw/api/comment/v1"
	inventoryV1 "shopgw/api/inventory/v1"
	productV1 "shopgw/api/product/v1"
	shopgwV1 "shopgw/api/shopgw/v1"
	"shopgw/internal/rpcclient"
)

var _ shopgwV1.ShopGwLogicer = (*shopGwClient)(nil)

type shopGwClient struct {
	productCli productV1.ProductClient
	inventoryCli inventoryV1.InventoryClient
	commentCli commentV1.CommentClient
}

// NewShopGwClient creating rpc clients
func NewShopGwClient() shopgwV1.ShopGwLogicer {
	return &shopGwClient{
		productCli: productV1.NewProductClient(rpcclient.GetProductRPCConn()),
		inventoryCli: inventoryV1.NewInventoryClient(rpcclient.GetInventoryRPCConn()),
		commentCli: commentV1.NewCommentClient(rpcclient.GetCommentRPCConn()),
	}
}

func (c *shopGwClient) GetDetailsByProductID(ctx context.Context, req *shopgwV1.GetDetailsByProductIDRequest) (*shopgwV1.GetDetailsByProductIDReply, error) {
	productRep, err := c.productCli.GetByID(ctx, &productV1.GetByIDRequest{
		Id: req.ProductID,
	})
	if err ! = nil {
		return nil, err
	}

	inventoryRep, err := c.inventoryCli.GetByID(ctx, &inventoryV1.GetByIDRequest{
		Id: productRep.InventoryID,
	})
	if err ! = nil {
		return nil, err
	}

	commentRep, err := c.commentCli.ListByProductID(ctx, &commentV1.ListByProductIDRequest{
		ProductID: req,
	})
	if err ! = nil {
		return nil, err
	}

	return &shopgwV1.GetDetailsByProductIDReply{
		ProductDetail: productRep.ProductDetail,
		InventoryDetail: inventoryRep.InventoryDetail,
		CommentDetails: commentRep,
	}, nil
}
```

Start the service again.

```bash
# Compile and run services
make run
```

When visiting http://localhost:8080/apis/swagger/index.html in a browser, the request returns a 503 error (service unavailable) because none of the three rpc services **product**, **inventory**, and **comment** are running yet.

The code for all three rpc services **product**, **inventory** and **comment** are not available yet, so how to start them properly. The proto files for these three rpc services are already available, and it is easy to generate the code and start the services according to the section **4.2 Creating rpc services from proto files** steps.

<br>

### 4.4 Summary

The generation of rpc service code is based on both mysql and proto files, according to the proto file method in addition to support the generation of rpc service code, also support the generation of rpc gateway service (http) code.

- The rpc service code generated according to mysql includes CRUD method logic code and proto code for each data table, subsequently if you want to add new methods, just define them in the proto file, manually add business logic code can refer to CRUD logic code.
- Generate rpc service code based on proto file does not include operational database code, but you can use `sponge web dao` command to generate operational database code, generate service template code based on proto file, and populate business logic code in the template code.
- Generate rpc gateway service code based on proto file, api definition in proto file, generate service template code based on proto file, populate business logic code in template code, use in combination with `sponge micro rpc-cli` command.

According to the actual scenario choose to generate the corresponding service code, if the main is to add, delete and check the data table, according to mysql generate rpc service can write less code; if more custom methods, according to the proto generate rpc service is more appropriate; rpc to http use rpc gateway service.

<br><br>

## 5 Service governance

### 5.1 Trace

#### 5.1.1 Starting jaeger and elasticsearch services

Use jaeger for tracing and elasticsearch for storage, and start both services locally using [docker-compose](https://github.com/docker/compose/releases).

**(1) elasticsearch service**

This is the [startup script for the elasticsearch service](https://github.com/zhufuyi/sponge/tree/main/test/server/elasticsearch), and the **.env** file is the startup configuration for elasticsearch, starting the elasticsearch service.

> docker-compose up -d

<br>

**(2) jaeger services**

This is the [jaeger service startup script](https://github.com/zhufuyi/sponge/tree/main/test/server/jaeger), the **.env** file is to configure the jaeger information and start the jaeger service at.

> docker-compose up -d

Visit the jaeger query home page in your browser [http://localhost:16686](http://localhost:16686) .

<br>

#### 5.1.2 Single Service Trace Example

Using the http service code created in **Section 3.1.2** as an example, modify the configuration file `configs/edusys.yml` to enable trace (field enableTrace) and fill in the jaeger configuration information.

If you want to trace redis, enable redis caching, change the cache type field **cacheType** value to redis and configure the redis configuration, and start the redis service locally using docker, which is [redis service startup script](https://github.com/zhufuyi/ sponge/tree/main/test/server/redis).

Start the http service.

```bash
# Compile and run services
make run
```

Copy [http://localhost:8080/swagger/index.html](http://localhost:8080/apis/swagger/index.html) to the browser to access the swagger home page to request the get query as an example, requesting the same id twice in a row, with the trace shown in Figure 5-1.

![one-server-trace](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/one-server-trace.jpg)
*Figure 5-1 single service trace diagram*

<br>

From the figure you can see that the first request has 4 spans, which are.

- Request api `/api/v1/teacher/1`
- Querying redis
- Query mysql
- Setting up the redis cache

It means that the first request looks up from redis, does not hit the cache, then reads the data from mysql and finally places the cache.

The second request has only 2 spans, which are.

- Request api `/api/v1/teacher/1`
- Querying redis

It means that the second request hits the cache directly, with less querying mysql and setting up the cache process than the first.

These spans are automatically generated, many times you need to manually add custom spans, add span example.

```go
import "github.com/zhufuyi/sponge/pkg/tracer"

tags := map[string]interface{}{"foo": "bar"}
_, span := tracer.NewSpan(ctx, "spanName", tags)  
defer span.End()
```

<br>

#### 5.1.3 Multi-Service Trace Example

Take the rpc gateway service code generated by **section 4.3** as an example, a total of four services **shopgw**, **product**, **inventory**, **comment**, modify the configuration of each of the four services (in the configs directory), turn on trace, and fill in the jaeger configuration information.

Find the template file in the **internal/service** directory of the **product**, **inventory**, and **comment** services, populate the code in place of `panic("improve me")` to make the code execute properly, and manually add a **span** that Add a random delay.

Start the four services **shopgw**, **product**, **inventory**, and **comment**, and visit [http://localhost:8080/apis/swagger/index.html](http://localhost:8080/apis/swagger/index.html) in the browser to execute a get request, and the trace page is shown in Figure 5-2.

![multi-servers-trace](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/multi-servers-trace.jpg)
*Figure 5-2 multi-service trace diagram*

As you can see from the figure there are 10 spans with the main links.

- request api `/api/v1/detail`
- the shopgw service calls the product client
- product's rpc server
- mockDAO added manually in the product service
- shopgw service calls the inventory client
- inventory's rpc server
- manually added mockDAO in the inventory service
- The shopgw service calls the comment client
- comment's rpc server
- mockDAO added manually in the comment service

The shopgw service calls **product**, **inventory**, and **comment** services serially to get data, but in practice it would be more time efficient to call them in parallel, but be careful to control the number of concurrent processes.

<br>

### 5.2 Monitoring

#### 5.2.1 Starting Prometheus and Grafana Services

Use [Prometheus](https://prometheus.io/docs/introduction/overview) for collecting metrics and [Grafana](https://grafana.com/docs/) for display, and start both services locally using docker.

**(1) prometheus services**

This is the [prometheus service startup script](https://github.com/zhufuyi/sponge/tree/main/test/server/monitor/prometheus) that starts the prometheus service.

> docker-compose up -d

Visit the prometheus homepage in your browser [http://localhost:9090](http://localhost:9090/) .

<br>

**(2) grafana services**

This is the [grafana service startup script](https://github.com/zhufuyi/sponge/tree/main/test/server/monitor/grafana) that starts the grafana service.

> docker-compose up -d

Visit the grafana main page [http://localhost:33000](http://localhost:33000) in your browser, set the datasource for prometheus `http://192.168.3.37:9090`, remember the datasource name for prometheus (here it is **Prometheus**), and the **datasource** value for the json imported into the monitoring panel later should be the same.

<br>

#### 5.2.2 http service monitoring

As an example, the http service code generated by **Section 3.1.2** provides the indicator api [http://localhost:8080/metrics](http://localhost:8080/metrics) by default.

**(1) Add monitoring targets to prometheus**

Open the prometheus configuration file prometheus.yml and add the acquisition target.

```bash
  - job_name: 'http-edusys'
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:8080']
```

Note: If you use vim to modify the prometheus.yml file, you must change the file prometheus.yml permissions to `0777` before modifying it, otherwise the modification configuration file cannot be synchronized to the container.

Execute the request to make the prometheus configuration take effect `curl -X POST http://localhost:9090/-/reload`, wait a moment, and then visit [http://localhost:9090/targets](http://localhost:9090/targets) in your browser to check if the newly added capture target takes effect.

<br>

**(2) Adding a monitoring panel to grafana**

Import the [http monitoring panel](https://github.com/zhufuyi/sponge/blob/main/pkg/gin/middleware/metrics/gin_grafana.json) into grafana, if no data is displayed in the monitoring interface, check that the data source in the json name is the same as the grafana configuration prometheus data source name.

<br>

**(3) Compression test api, observation of monitoring data**

Using the [wrk](https://github.com/wg/wrk) tool to pressure test the api

```bash
# api 1
wrk -t2 -c10 -d10s http://192.168.3.27:8080/api/v1/teacher/1

# api 2
wrk -t2 -c10 -d10s http://192.168.3.27:8080/api/v1/course/1
```

The monitoring interface is shown in Figure 5-3.

![http-grafana](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/http-grafana.jpg)
*Figure 5-3 http service monitoring diagram*

<br>

#### 5.2.3 rpc service monitoring

As an example, the rpc service code generated by **Section 4.1.1** provides the indicator api [http://localhost:8283/metrics](http://localhost:8283/metrics) by default.

**(1) Add monitoring targets to prometheus**

Open the prometheus configuration file prometheus.yml and add the acquisition target.

```bash
  - job_name: 'rpc-server-edusys'
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:8283']
```

Execute the request to make the prometheus configuration take effect `curl -X POST http://localhost:9090/-/reload`, wait a moment, and then visit [http://localhost:9090/targets](http://localhost:9090/targets) in your browser to check if the newly added capture target is in effect.

<br>

**(2) Adding a monitoring panel to grafana**

Import the [rpc server monitoring panel](https://github.com/zhufuyi/sponge/blob/main/pkg/grpc/metrics/server_grafana.json) into grafana, and if no data is displayed in the monitoring interface, check that the data source name in the json is the same as the grafana configuration prometheus data source name is the same.

<br>

**(3) Compression test rpc methods, observation of monitoring data**

Open the `internal/service/teacher_client_test.go` file using **Goland** or **VS Code** and test each method under **Test_teacherService_methods** or **Test_teacherService_benchmark**.

The monitoring result is shown in Figure 5-4.
![rpc-grafana](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/rpc-grafana.jpg)
*Figure 5-4 rpc server monitoring diagram*

<br>

The above is the monitoring of the rpc server, and the monitoring of the rpc client is similar, [rpc client monitoring panel](https://github.com/zhufuyi/sponge/blob/main/pkg/grpc/metrics/client_grafana.json) .

<br>

#### 5.2.4 Automatic addition and removal of monitoring targets in prometheus

Prometheus supports dynamic configuration using consul's service registration and discovery to automatically add and remove monitoring targets.

Start the consul service locally, this is the [consul service startup script](https://github.com/zhufuyi/sponge/tree/main/test/server/consul)

Open prometheus config prometheus.yml and add the consul configuration.

```yaml
  - job_name: 'consul-micro-exporter'
    consul_sd_configs:
      - server: 'localhost:8500'
        services: []  
    relabel_configs:
      - source_labels: [__meta_consul_tags]
        regex: . *edusys.*
        action: keep
      - regex: __meta_consul_service_metadata_(. +)
        action: labelmap
```

Execute the request to make the prometheus configuration take effect `curl -X POST http://localhost:9090/-/reload`.

After the consul service discovery is configured in prometheus, the address information of the service is then pushed to consul, and the push information edusys_exporter.json file reads as follows.

```json
{
  "ID": "edusys-exporter",
  "Name": "edusys",
  "Tags": [
    "edusys-exporter"
  ],
  "Address": "localhost",
  "Port": 8283,
  "Meta": {
    "env": "dev",
    "project": "edusys"
  },
  "EnableTagOverride": false,
  "Check": {
    "HTTP": "http://localhost:8283/metrics",
    "Interval": "10s"
  },
  "Weights": {
    "Passing": 10,
    "Warning": 1
  }
}
```

> curl -XPUT --data @edusys_exporter.json http://localhost:8500/v1/agent/service/register

Wait a moment, then open [http://localhost:9090/targets](http://localhost:9090/targets) in your browser to check if the newly added acquisition target is in effect. Then close the service, wait a while, and check if the acquisition target is automatically removed.

<br>

For your own services, you usually submit information to consul at the same time you start the service, convert edusys_exporter.json to a go struct, and call the http client inside the program to submit it to consul.

<br>

### 5.3 Collecting go program profiles

Usually use the pprof tool to find and locate program problems, especially online go program problems can automatically save the program run site (profile), and then use the tool pprof analysis to locate the problem.

The sponge generated service supports **http api** and **system signal notification** to collect profiles, the system signal notification method is enabled by default, just use one.

<br>

#### 5.3.1 Collecting profiles via http

Through the http api way to collect profile is closed by default, if you need to open, modify the configuration of the field `enableHTTPProfile` to true, usually used in development or testing, if the line open will have a little performance loss, according to the actual situation whether to open the use.

The default route `/debug/pprof`, combined with the **go tool pprof** tool, allows you to analyze the current running status of your program at any moment.

<br>

#### 5.3.2 Notification of acquisition profiles via system signals

Using the http api, the program background has been regularly recording profile-related information, etc., the vast majority of the time will not read these profiles, can be improved, only when needed and then start collecting profiles, automatically shut down after collection, sponge generated services support listening to the system signal to start and stop collecting profiles, the default uses **SIGTRAP**(5) system signals (suggested to be changed to SIGUSR1, not supported in windows environment), sends signals to the service.

```bash
# View service pid by name (second column)
ps aux | grep service name

# Send a signal to the service
kill -trap pid value

# kill -usr1 pid value
```

After the service receives the system signal notification, it starts to collect the profile and save it to the `/tmp/service_name_profile` directory, the default collection length is 60 seconds, after 60 seconds it automatically stops collecting the profile, if you only want to collect 30 seconds, send the first signal to start collecting, about 30 seconds later send the second signal to indicate that it stops collecting the profile, similar to switch. Default acquisition **cpu**, **memory**, **goroutine**, **block**, **mutex**, **threadcreate** six types of profiles, file format `date_time_pid_service_name_profile_type.out`, example.

```
xxx221809_58546_edusys_cpu.out
xxx221809_58546_edusys_mem.out
xxx221809_58546_edusys_goroutine.out
xxx221809_58546_edusys_block.out
xxx221809_58546_edusys_mutex.out
xxx221809_58546_edusys_threadcreate.out
```

Because the profile file of trace is relatively large, it is not captured by default and can be turned on to capture trace according to actual needs (call prof.EnableTrace() when the service starts).

Once the offline files are obtained, they are analysed using the pprof tool using an interactive or interface approach.

```bash
# Interactive
go tool pprof [options] source

# web ui
go tool pprof -http=[host]:[port] [options] source
```

<br>

#### 5.3.3 Automatic profile capture

All of the above are manual profile collection, and it is usually desirable to automatically collect profiles when problems occur. sponge-generated services support automatic profile collection by default, and are implemented in conjunction with the alerting feature of resource statistics. alerting conditions.

- Record the program's cpu usage 3 times in a row (once per minute by default) and trigger an alarm when the average usage exceeds 80% for 3 times.
- Record the program's use of physical memory for 3 consecutive times (once per minute by default), and trigger an alarm when the average system memory usage exceeds 80% for 3 times.
- If the alarm threshold is continuously exceeded, the default interval is 15 minutes between alarms.

When the alarm is triggered, the program internally calls the kill function to send system signal to notify the acquisition profile, and the acquired profile file is saved to the `/tmp/service name_profile` directory, which is actually the basis of **notifying the acquisition profile by system signal** to change the manual trigger to automatic trigger, even in the middle of the night the program's cpu or memory is too high, the next day you can also analyze the profile to find out where the program is causing the cpu or memory to be too high.

Note: Automatic profile collection is not suitable for windows environment.

<br>

### 5.4 Registration Centre

The sponge-generated services support the [Nacos](https://nacos.io/zh-cn/docs/v2/what-is-nacos.html) configuration center by default. The role of the configuration center is to unify the configuration management of different environments and services, effectively solving the shortcomings of the ground static configuration.

Start the nacos service locally, this is the [nacos service startup configuration](https://github.com/zhufuyi/sponge/tree/main/test/server/nacos), after starting the nacos service, open the administration interface in your browser http://localhost:8848/nacos/index.html , login to the account password to enter the main interface.

Using the http service code generated by **Section 3.1.2** as an example using Configuration Center nacos, create a namespace `edusys` in the nacos interface, then create a new configuration with a Data ID value of `edusys.yml`, a Group value of `dev`, and a configuration content value of the `configs/edusys.yml` file contents, as shown in Figure 5-3.

![nacos-config](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/nacos-config.jpg)
*Figure 5-3 nacos add service configuration diagram*

Open the configuration center file `configs/edusys_cc.yml` in the edusys directory and fill in the nacos configuration information.

```yaml
# Generate the go struct command: sponge config --server-dir=./serverDir

# nacos settings
nacos:
  ipAddr: "192.168.3.37" # server address
  port: 8848 # listening port
  scheme: "http" # http or https
  contextPath: "/nacos" # path
  namespaceID: "ecfe0595-cae3-43a2- 9e47-216dc92207f9" # namespace id
  group: "dev" # group name: dev, prod, test
  dataID: "edusys.yml" # config file id
  format: "yaml" # configuration file type: json,yaml,toml
```

Compiling and starting edusys services.

```bash
# Switch to the main.go location
cdd cmd/edusys

# Compilation
go build

# Run
./edusys -enable-cc -c=../../configs/edusys_cc.yml
```

The start service parameter `-c` indicates that a configuration file is specified, and the parameter `-enable-cc` indicates that the configuration is obtained from the configuration center.

<br>

### 5.5 Rate limiter and circuit breaker

The service created by sponge supports rate limiter and circuit breaker, which is off by default. Open the service configuration file and modify the field **enableLimit** to a value of `true` to enable rate limiter, and modify the field **enableCircuitBreaker** to `true` to enable circuit breaker.

Rate limiter and circuit breaker use a third-party library [aegis](https://github.com/go-kratos/aegis), which adapts according to system resources and error rates. Since different servers have different processing capabilities and parameters are not well set, using adaptive parameters avoids the trouble of manually setting parameters for each service.

<br><br>

## 6 Continuous Integration Deployment

The services created by sponge support build and deployment in [jenkins](https://www.jenkins.io/doc/), the deployment target can be docker, [k8s](https://kubernetes.io/docs/home/) , the deployment script is in the **deployments** directory, the following is an example of deployment to k8s using jenkins.

### 6.1 Building the jenkins-go platform

In order to be able to compile go code in a container, you need to build a jenkins-go image, which is already built [jenkins-go image](https://hub.docker.com/r/zhufuyi/jenkins-go/tags). If you want to build the jenkins-go image yourself, you can refer to the docker build script [Dokerfile](https://github.com/zhufuyi/sponge/blob/main/test/server/jenkins/Dockerfile)

After preparing the jenkins-go image, you also need to prepare a k8s cluster (there are many k8s cluster tutorials online), a k8s forensics file and a command line tool [kubectl](https://kubernetes.io/zh-cn/docs/tasks/tools/#kubectl) to ensure that you have permission to operate k8s in the jenkins-go container .

The jenkins-go startup script, docker-compose.yml, reads

```yaml
version: "3.7"
services:
  jenkins-go:
    image: zhufuyi/jenkins-go:2.37
    restart: always
    container_name: "jenkins-go"
    ports:
      - 38080:8080
    #- 50000:50000
    volumes:
      - $PWD/jenkins-volume:/var/jenkins_home
      # docker configuration
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker
      - /root/.docker/:/root/.docker/
      # k8s api configuration directory, including config file
      - /usr/local/bin/kubectl:/usr/local/bin/kubectl
      - /root/.kube/:/root/.kube/
      # go related tools
      - /opt/go/bin/golangci-lint:/usr/local/bin/golangci-lint
```

Start the jenkis-go service.

> docker-compose up -d

Visit [http://localhost:38080](http://localhost:38080) in your browser, the first time you start it you need the admin key (execute the command to get `docker exec jenkins-go cat /var/jenkins_home/secrets/initialAdminPassword`), then install the recommended plugins and set the admin account password, then install some plugins you need to use and some custom settings.

**(1) Installation of plug-ins**

```bash
# Chinese plugin
Locale

# Add parametric build plugins
Extended Choice Parameter

# Add git parameters plugin
Git Parameter

# Account management
Role-based Authorization Strategy
```

**(2) Set Chinese **

Click [Manage Jenkins] -> [Configure System] option, find the [Locale] option, enter [zh_CN], check the following options, and finally click [Apply].

**(3) Configuration of global parameters**

dashboard --> System Administration --> System Configuration --> Check the environment variables

Set the repository address of the container image.

```bash
# Development environment image repository
DEV_REGISTRY_HOST http://localhost:27070

# Test environment image repository
TEST_REGISTRY_HOST http://localhost:28080

# Production environment image repository
PROD_REGISTRY_HOST http://localhost:29090
```

<br>

### 6.2 Creating templates

A relatively simple way to create a new task for jenkins is to import an existing template when creating a new task and then modify the git repository address. The first time you use jenkins and don't have a template yet, you can create one by following these steps.

**(1) Create a new task**, as shown in Figure 6-1.

![create-job](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/createJob.jpg)
*Figure 6-1 Task creation screen diagram*

<br>

**(2) Parameterized configuration setting**, using the parameter name `GIT_parameter`, as shown in Figure 6-2.

![parametric-construction](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/paramSetting.jpg)
*Figure 6-2 Setting up the parametric build diagram*

<br>

**(3) Set up the pipeline**, as shown in Figure 6-3.

![pipeline](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/pipelineSetting.jpg)
*Figure 6-3 Setup pipeline screen diagram*

<br>

**(4) Construction project**

Click **Build with Parameters** on the left menu bar, and then select the branch or tag you want to branch or tag, as shown in Figure 6-4.

![start-build](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/building.jpg)
*Figure 6-4 Parametric build diagram*

<br>

### 6.3 Deploying to k8s

Take the edusys service in **Chapter 3.1.2** as an example, built and deployed to k8s using jenkins.

The first build of the service requires some prep work.

(1) Upload the edusys code to the code repository.

(2) Prepare a docker image repository and make sure the docker where jenkins-go is located has permission to upload images to the image repository.

(3) Ensure that you have permission to pull images from the mirror on the k8s cluster node, and execute the command to generate the key on the logged-in docker image repository server.

```bash
kubectl create secret generic docker-auth-secret \
    --from-file=.dockerconfigjson=/root/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson
```

(4) Create edusys related resources at k8s.

```bash
# Switch to directory
cd deployments/kubernetes

# Create namespace, name corresponds to spong create service parameter project-name
kubectl apply -f ./*namespace.yml

# Create configmap, service
kubectl apply -f ./*configmap.yml
kubectl apply -f ./*svc.yml
```

(5) If you want to use pinned notifications to view the build deployment results, open the **Jenkinsfile** file under the code base, find the field **tel_num** and fill in the mobile number, and find **access_token** and fill in the token value.

<br>

After the prep, create a new task (name edusys) in the jenkins interface, using the template created above (name sponge), then modify the git repository, save the task and start the parametric build, the result of the construction is shown in Figure 6-5.

![run-job](https://raw.githubusercontent.com/zhufuyi/sponge/main/assets/jenkins-build.jpg)
*Figure 6-5 jenkins build result diagram*

<br>

Use the command `kubectl get all -n edusys` to see the status of the edusys service running in k8s.

```
NAME                             READY   STATUS    RESTARTS   AGE
pod/edusys-dm-77b4bcccc5-8xt8v   1/1     Running   0          21m

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/edusys-svc   ClusterIP   10.108.31.220   <none>        8080/TCP   27m

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/edusys-dm   1/1     1            1           21m

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/edusys-dm-77b4bcccc5   1         1         1       21m
```

<br>

Test locally to see if it is accessible

```bash
# Proxy ports
kubectl port-forward --address=0.0.0.0 service/edusys-svc 8080:8080 -n edusys

# Requests
curl http://localhost:8080/api/v1/teacher/1
```

<br>

The services generated by sponge include a Jenkinsfile, build and upload image scripts, and k8s deployment scripts, which can be used basically without modifying the scripts, or you can modify the scripts to suit your scenario.

<br><br>

If it's useful to you, give it a star.
